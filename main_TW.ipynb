{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TypeWriter\n",
    "This notebook runs the TypeWriter method for predicting types of Python methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typewriter import config_TW\n",
    "from os.path import join, exists, isdir\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import result_proc\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from result_proc import copy_results, clean_output\n",
    "# Copying the results and cleaning the output of last run\n",
    "#copy_results('./output/reports/', './results/')\n",
    "\n",
    "# Delete all the files in the output folder\n",
    "#clean_output('./output/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Python projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gh_query import load_json, gen_json_file, find_current_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we only select Python projects that has `mypy` as one of its dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = load_json('./data/mypy-dependents-by-stars.json')\n",
    " \n",
    "gen_json_file('./data/py_projects_all.json', repos, find_current_repos('./data/full_dataset/', True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads selected repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = load_json('./data/py_projects_all.json')\n",
    "print(\"number of projects:\", len(repos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output folder and dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give a name to output directory. It'll be created automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './output_py_500/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_EMBEDDINGS_DIRECTORY = join(OUTPUT_DIR, 'embed')\n",
    "OUTPUT_DIRECTORY_TW = join(OUTPUT_DIR, 'funcs')\n",
    "AVAILABLE_TYPES_DIR = join(OUTPUT_DIR, 'avl_types')\n",
    "RESULTS_DIR = join(OUTPUT_DIR, \"results\")\n",
    "\n",
    "ML_INPUTS_PATH_TW = join(OUTPUT_DIR, \"ml_inputs\")\n",
    "ML_PARAM_TW_TRAIN = join(ML_INPUTS_PATH_TW, \"_ml_param_train.csv\")\n",
    "ML_PARAM_TW_TEST = join(ML_INPUTS_PATH_TW, \"_ml_param_test.csv\")\n",
    "ML_RET_TW_TRAIN = join(ML_INPUTS_PATH_TW, \"_ml_ret_train.csv\")\n",
    "ML_RET_TW_TEST = join(ML_INPUTS_PATH_TW, \"_ml_ret_test.csv\")\n",
    "\n",
    "VECTOR_OUTPUT_DIR_TW = join(OUTPUT_DIR, 'vectors')\n",
    "VECTOR_OUTPUT_TRAIN = join(VECTOR_OUTPUT_DIR_TW, \"train\")\n",
    "VECTOR_OUTPUT_TEST = join(VECTOR_OUTPUT_DIR_TW, \"test\")\n",
    "\n",
    "W2V_MODEL_TOKEN_DIR = join(OUTPUT_EMBEDDINGS_DIRECTORY, 'w2v_token_model.bin')\n",
    "W2V_MODEL_COMMENTS_DIR = join(OUTPUT_EMBEDDINGS_DIRECTORY, 'w2v_comments_model.bin')\n",
    "\n",
    "DATA_FILE_TW = join(ML_INPUTS_PATH_TW, \"_all_data.csv\")\n",
    "\n",
    "LABEL_ENCODER_PATH_TW = join(ML_INPUTS_PATH_TW, \"label_encoder.pkl\")\n",
    "TYPES_FILE_TW = join(ML_INPUTS_PATH_TW, \"_most_frequent_types.csv\")\n",
    "\n",
    "dirs = [OUTPUT_EMBEDDINGS_DIRECTORY, OUTPUT_DIRECTORY_TW, AVAILABLE_TYPES_DIR, RESULTS_DIR,\n",
    "        ML_INPUTS_PATH_TW, VECTOR_OUTPUT_DIR_TW, VECTOR_OUTPUT_TRAIN, VECTOR_OUTPUT_TEST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isdir(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "config_TW.create_dirs(dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extracting functions\n",
    "Extract natural language information and preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltpy.preprocessing.pipeline import Pipeline\n",
    "p = Pipeline('./data/full_dataset/', OUTPUT_DIRECTORY_TW, AVAILABLE_TYPES_DIR)\n",
    "p.run_pipeline_manual(repos, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates dataframe or loads an existing one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltpy.input_preparation.generate_df import list_files, parse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached copy\n"
     ]
    }
   ],
   "source": [
    "if config_TW.CACHE_TW and os.path.exists(DATA_FILE_TW):\n",
    "    print(\"Loading cached copy\")\n",
    "    df = pd.read_csv(DATA_FILE_TW)\n",
    "else:\n",
    "    DATA_FILES = list_files(OUTPUT_DIRECTORY_TW)\n",
    "    print(\"Found %d datafiles\" % len(DATA_FILES))\n",
    "    #print(DATA_FILES)\n",
    "    df = parse_df(DATA_FILES, batch_size=4098)\n",
    "    print(\"Dataframe loaded writing it to CSV\")\n",
    "    df.to_csv(DATA_FILE_TW, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial dataset before processing parameter and returns data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>repo</th>\n",
       "      <th>file</th>\n",
       "      <th>has_type</th>\n",
       "      <th>name</th>\n",
       "      <th>docstring</th>\n",
       "      <th>func_descr</th>\n",
       "      <th>arg_names</th>\n",
       "      <th>arg_types</th>\n",
       "      <th>arg_descrs</th>\n",
       "      <th>return_type</th>\n",
       "      <th>return_expr</th>\n",
       "      <th>return_descr</th>\n",
       "      <th>args_occur</th>\n",
       "      <th>arg_names_len</th>\n",
       "      <th>arg_types_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/setup.py</td>\n",
       "      <td>False</td>\n",
       "      <td>get requirement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['path']</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['buff read splitlines']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/setup.py</td>\n",
       "      <td>False</td>\n",
       "      <td>get long description</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['buff read']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/setup.py</td>\n",
       "      <td>False</td>\n",
       "      <td>get version</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['mo group']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>False</td>\n",
       "      <td>inner map</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['obj']</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['type obj map inner map obj', 'dict map inner...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>False</td>\n",
       "      <td>map nest</td>\n",
       "      <td>structure preserve nested map apply fn object ...</td>\n",
       "      <td>structure preserve nested map</td>\n",
       "      <td>['fn', 'structure', 'cond']</td>\n",
       "      <td>['', '', '']</td>\n",
       "      <td>['', '', '']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['inner map structure']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>True</td>\n",
       "      <td>merge dicts</td>\n",
       "      <td>merge dicts assert key overlap parameter dicts...</td>\n",
       "      <td>merge dicts assert key overlap</td>\n",
       "      <td>['dicts']</td>\n",
       "      <td>['dict']</td>\n",
       "      <td>['arbitrary number dicts']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['kwargs']</td>\n",
       "      <td>merge dict</td>\n",
       "      <td>['mappable dicts']</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>True</td>\n",
       "      <td>enhance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['args']</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>Callable</td>\n",
       "      <td>['newfn', 'newwrapper']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['hasattr args']</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>True</td>\n",
       "      <td>biwrap</td>\n",
       "      <td>allow optional keyword argument low level deco...</td>\n",
       "      <td>allow optional keyword argument low level deco...</td>\n",
       "      <td>['wrapper']</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>Callable</td>\n",
       "      <td>['enhance']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>False</td>\n",
       "      <td>be valid untransformed name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['cl', 'name']</td>\n",
       "      <td>['', '']</td>\n",
       "      <td>['', '']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['match be not none and match transform be none']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['match cl name', 'match name']</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>False</td>\n",
       "      <td>be valid name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['cl', 'name']</td>\n",
       "      <td>['', '']</td>\n",
       "      <td>['', '']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['match be not none']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['match cl name', 'match name']</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>False</td>\n",
       "      <td>init</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['self', 'path', 'transform name', 'untransfom...</td>\n",
       "      <td>['', '', '', '']</td>\n",
       "      <td>['', '', '', '']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['self pathself untransformed name untransfomr...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>False</td>\n",
       "      <td>from name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['cl', 'name']</td>\n",
       "      <td>['', '']</td>\n",
       "      <td>['', '']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['cls path match transform match name']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['match cl name r enot cl valid name', 'split ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>False</td>\n",
       "      <td>original name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['self']</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['format self transform name self untransforme...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>False</td>\n",
       "      <td>full original name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['self']</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['join self path self original name']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>False</td>\n",
       "      <td>full untransformed name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['self']</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['join self path self untransformed name']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>False</td>\n",
       "      <td>be transform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['self']</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['self transform name be not none']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>False</td>\n",
       "      <td>repr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['self']</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['name part of format self full original name']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/ut...</td>\n",
       "      <td>False</td>\n",
       "      <td>replace transform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['self', 'transform name']</td>\n",
       "      <td>['', '']</td>\n",
       "      <td>['', '']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['self class self path transform name self unt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/as...</td>\n",
       "      <td>False</td>\n",
       "      <td>uncompile</td>\n",
       "      <td>uncompile codeobj source filename mode flag fi...</td>\n",
       "      <td>uncompile codeobj source filename mode flag fi...</td>\n",
       "      <td>['c']</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['source filename exec c co flag py cf mask fi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['c co flagsif c co nameif c co filename getfi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/as...</td>\n",
       "      <td>False</td>\n",
       "      <td>fixnames</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['name']</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['tuple privateprefix name if isprivate name e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/as...</td>\n",
       "      <td>False</td>\n",
       "      <td>recompile</td>\n",
       "      <td>recompile output uncompile back code object so...</td>\n",
       "      <td>recompile output uncompile back code object so...</td>\n",
       "      <td>['source', 'filename', 'mode', 'flag', 'firstl...</td>\n",
       "      <td>['', '', '', '', '', '']</td>\n",
       "      <td>['', '', '', '', '', '']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['c']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['isinstance source ast', 'source filename mod...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/as...</td>\n",
       "      <td>False</td>\n",
       "      <td>parse snippet</td>\n",
       "      <td>like ast parse accepts indent code snippet lin...</td>\n",
       "      <td>like ast parse accepts indent code snippet lin...</td>\n",
       "      <td>['source', 'filename', 'mode', 'flag', 'firstl...</td>\n",
       "      <td>['', '', '', '', '', '']</td>\n",
       "      <td>['', '', '', '', '', '']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['a']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['prefix source args', 'args filename mode fla...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/as...</td>\n",
       "      <td>False</td>\n",
       "      <td>init</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['self']</td>\n",
       "      <td>['']</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['auto name visitor self self random variable ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/as...</td>\n",
       "      <td>False</td>\n",
       "      <td>visit assign</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['self', 'node']</td>\n",
       "      <td>['', '']</td>\n",
       "      <td>['', '']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['return']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['self random variable namesself generic visit...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pymc-devs</td>\n",
       "      <td>pymc4</td>\n",
       "      <td>./data/training_repos/pymc-devs/pymc4/pymc4/as...</td>\n",
       "      <td>False</td>\n",
       "      <td>parse random variable name</td>\n",
       "      <td>parse random variable name yielded distributio...</td>\n",
       "      <td>parse random variable name yielded distributio...</td>\n",
       "      <td>['model']</td>\n",
       "      <td>['']</td>\n",
       "      <td>['model function']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['visitor random variable name']</td>\n",
       "      <td>random variable name list list random variable...</td>\n",
       "      <td>['uncompile model code']</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author   repo                                               file  \\\n",
       "0   pymc-devs  pymc4     ./data/training_repos/pymc-devs/pymc4/setup.py   \n",
       "1   pymc-devs  pymc4     ./data/training_repos/pymc-devs/pymc4/setup.py   \n",
       "2   pymc-devs  pymc4     ./data/training_repos/pymc-devs/pymc4/setup.py   \n",
       "3   pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "4   pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "5   pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "6   pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "7   pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "8   pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "9   pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "10  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "11  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "12  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "13  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "14  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "15  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "16  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "17  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/ut...   \n",
       "18  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/as...   \n",
       "19  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/as...   \n",
       "20  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/as...   \n",
       "21  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/as...   \n",
       "22  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/as...   \n",
       "23  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/as...   \n",
       "24  pymc-devs  pymc4  ./data/training_repos/pymc-devs/pymc4/pymc4/as...   \n",
       "\n",
       "    has_type                         name  \\\n",
       "0      False              get requirement   \n",
       "1      False         get long description   \n",
       "2      False                  get version   \n",
       "3      False                    inner map   \n",
       "4      False                     map nest   \n",
       "5       True                  merge dicts   \n",
       "6       True                      enhance   \n",
       "7       True                       biwrap   \n",
       "8      False  be valid untransformed name   \n",
       "9      False                be valid name   \n",
       "10     False                         init   \n",
       "11     False                    from name   \n",
       "12     False                original name   \n",
       "13     False           full original name   \n",
       "14     False      full untransformed name   \n",
       "15     False                 be transform   \n",
       "16     False                         repr   \n",
       "17     False            replace transform   \n",
       "18     False                    uncompile   \n",
       "19     False                     fixnames   \n",
       "20     False                    recompile   \n",
       "21     False                parse snippet   \n",
       "22     False                         init   \n",
       "23     False                 visit assign   \n",
       "24     False   parse random variable name   \n",
       "\n",
       "                                            docstring  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4   structure preserve nested map apply fn object ...   \n",
       "5   merge dicts assert key overlap parameter dicts...   \n",
       "6                                                 NaN   \n",
       "7   allow optional keyword argument low level deco...   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18  uncompile codeobj source filename mode flag fi...   \n",
       "19                                                NaN   \n",
       "20  recompile output uncompile back code object so...   \n",
       "21  like ast parse accepts indent code snippet lin...   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24  parse random variable name yielded distributio...   \n",
       "\n",
       "                                           func_descr  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                       structure preserve nested map   \n",
       "5                      merge dicts assert key overlap   \n",
       "6                                                 NaN   \n",
       "7   allow optional keyword argument low level deco...   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18  uncompile codeobj source filename mode flag fi...   \n",
       "19                                                NaN   \n",
       "20  recompile output uncompile back code object so...   \n",
       "21  like ast parse accepts indent code snippet lin...   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24  parse random variable name yielded distributio...   \n",
       "\n",
       "                                            arg_names  \\\n",
       "0                                            ['path']   \n",
       "1                                                  []   \n",
       "2                                                  []   \n",
       "3                                             ['obj']   \n",
       "4                         ['fn', 'structure', 'cond']   \n",
       "5                                           ['dicts']   \n",
       "6                                            ['args']   \n",
       "7                                         ['wrapper']   \n",
       "8                                      ['cl', 'name']   \n",
       "9                                      ['cl', 'name']   \n",
       "10  ['self', 'path', 'transform name', 'untransfom...   \n",
       "11                                     ['cl', 'name']   \n",
       "12                                           ['self']   \n",
       "13                                           ['self']   \n",
       "14                                           ['self']   \n",
       "15                                           ['self']   \n",
       "16                                           ['self']   \n",
       "17                         ['self', 'transform name']   \n",
       "18                                              ['c']   \n",
       "19                                           ['name']   \n",
       "20  ['source', 'filename', 'mode', 'flag', 'firstl...   \n",
       "21  ['source', 'filename', 'mode', 'flag', 'firstl...   \n",
       "22                                           ['self']   \n",
       "23                                   ['self', 'node']   \n",
       "24                                          ['model']   \n",
       "\n",
       "                   arg_types                  arg_descrs return_type  \\\n",
       "0                       ['']                        ['']         NaN   \n",
       "1                         []                          []         NaN   \n",
       "2                         []                          []         NaN   \n",
       "3                       ['']                        ['']         NaN   \n",
       "4               ['', '', '']                ['', '', '']         NaN   \n",
       "5                   ['dict']  ['arbitrary number dicts']         NaN   \n",
       "6                       ['']                        ['']    Callable   \n",
       "7                       ['']                        ['']    Callable   \n",
       "8                   ['', '']                    ['', '']         NaN   \n",
       "9                   ['', '']                    ['', '']         NaN   \n",
       "10          ['', '', '', '']            ['', '', '', '']         NaN   \n",
       "11                  ['', '']                    ['', '']         NaN   \n",
       "12                      ['']                        ['']         NaN   \n",
       "13                      ['']                        ['']         NaN   \n",
       "14                      ['']                        ['']         NaN   \n",
       "15                      ['']                        ['']         NaN   \n",
       "16                      ['']                        ['']         NaN   \n",
       "17                  ['', '']                    ['', '']         NaN   \n",
       "18                      ['']                        ['']         NaN   \n",
       "19                      ['']                        ['']         NaN   \n",
       "20  ['', '', '', '', '', '']    ['', '', '', '', '', '']         NaN   \n",
       "21  ['', '', '', '', '', '']    ['', '', '', '', '', '']         NaN   \n",
       "22                      ['']                        ['']         NaN   \n",
       "23                  ['', '']                    ['', '']         NaN   \n",
       "24                      ['']          ['model function']         NaN   \n",
       "\n",
       "                                          return_expr  \\\n",
       "0                            ['buff read splitlines']   \n",
       "1                                       ['buff read']   \n",
       "2                                        ['mo group']   \n",
       "3   ['type obj map inner map obj', 'dict map inner...   \n",
       "4                             ['inner map structure']   \n",
       "5                                          ['kwargs']   \n",
       "6                             ['newfn', 'newwrapper']   \n",
       "7                                         ['enhance']   \n",
       "8   ['match be not none and match transform be none']   \n",
       "9                               ['match be not none']   \n",
       "10                                                 []   \n",
       "11            ['cls path match transform match name']   \n",
       "12  ['format self transform name self untransforme...   \n",
       "13              ['join self path self original name']   \n",
       "14         ['join self path self untransformed name']   \n",
       "15                ['self transform name be not none']   \n",
       "16    ['name part of format self full original name']   \n",
       "17  ['self class self path transform name self unt...   \n",
       "18  ['source filename exec c co flag py cf mask fi...   \n",
       "19  ['tuple privateprefix name if isprivate name e...   \n",
       "20                                              ['c']   \n",
       "21                                              ['a']   \n",
       "22                                                 []   \n",
       "23                                         ['return']   \n",
       "24                   ['visitor random variable name']   \n",
       "\n",
       "                                         return_descr  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                          merge dict   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18                                                NaN   \n",
       "19                                                NaN   \n",
       "20                                                NaN   \n",
       "21                                                NaN   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24  random variable name list list random variable...   \n",
       "\n",
       "                                           args_occur  arg_names_len  \\\n",
       "0                                                  []              1   \n",
       "1                                                  []              0   \n",
       "2                                                  []              0   \n",
       "3                                                  []              1   \n",
       "4                                                  []              3   \n",
       "5                                  ['mappable dicts']              1   \n",
       "6                                    ['hasattr args']              1   \n",
       "7                                                  []              1   \n",
       "8                     ['match cl name', 'match name']              2   \n",
       "9                     ['match cl name', 'match name']              2   \n",
       "10  ['self pathself untransformed name untransfomr...              4   \n",
       "11  ['match cl name r enot cl valid name', 'split ...              2   \n",
       "12                                                 []              1   \n",
       "13                                                 []              1   \n",
       "14                                                 []              1   \n",
       "15                                                 []              1   \n",
       "16                                                 []              1   \n",
       "17                                                 []              2   \n",
       "18  ['c co flagsif c co nameif c co filename getfi...              1   \n",
       "19                                                 []              1   \n",
       "20  ['isinstance source ast', 'source filename mod...              6   \n",
       "21  ['prefix source args', 'args filename mode fla...              6   \n",
       "22  ['auto name visitor self self random variable ...              1   \n",
       "23  ['self random variable namesself generic visit...              2   \n",
       "24                           ['uncompile model code']              1   \n",
       "\n",
       "    arg_types_len  \n",
       "0               1  \n",
       "1               0  \n",
       "2               0  \n",
       "3               1  \n",
       "4               3  \n",
       "5               1  \n",
       "6               1  \n",
       "7               1  \n",
       "8               2  \n",
       "9               2  \n",
       "10              4  \n",
       "11              2  \n",
       "12              1  \n",
       "13              1  \n",
       "14              1  \n",
       "15              1  \n",
       "16              1  \n",
       "17              2  \n",
       "18              1  \n",
       "19              1  \n",
       "20              6  \n",
       "21              6  \n",
       "22              1  \n",
       "23              2  \n",
       "24              1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                              ['path']\n",
       "1                                                    []\n",
       "2                                                    []\n",
       "3                                               ['obj']\n",
       "4                           ['fn', 'structure', 'cond']\n",
       "5                                             ['dicts']\n",
       "6                                              ['args']\n",
       "7                                           ['wrapper']\n",
       "8                                        ['cl', 'name']\n",
       "9                                        ['cl', 'name']\n",
       "10    ['self', 'path', 'transform name', 'untransfom...\n",
       "11                                       ['cl', 'name']\n",
       "12                                             ['self']\n",
       "13                                             ['self']\n",
       "14                                             ['self']\n",
       "15                                             ['self']\n",
       "16                                             ['self']\n",
       "17                           ['self', 'transform name']\n",
       "18                                                ['c']\n",
       "19                                             ['name']\n",
       "Name: arg_names, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['arg_names'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats of the repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of source files: \", len(df.file.unique()))\n",
    "print(\"Number of functions: \", len(df.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of functions with comments: \",\n",
    "      df[(~df['return_descr'].isnull()) | (~df['func_descr'].isnull())].shape[0])\n",
    "print(\"Number of functions with return types: \", df['return_type'].count())\n",
    "print(\"Number of functions with both: \",\n",
    "      df[((~df['return_descr'].isnull()) | (~df['func_descr'].isnull())) & (~df['return_type'].isnull())].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the intial dataset based on source files. Later on, we use this to split the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_files, test_files = train_test_split(pd.DataFrame(df['file'].unique(), columns=['file']),\n",
    "                                           test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['file'].isin(train_files.to_numpy().flatten())]\n",
    "print(\"Number of functions in train set: \", df_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[df['file'].isin(test_files.to_numpy().flatten())]\n",
    "print(\"Number of functions in test set: \", df_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typewriter import prepocessing\n",
    "reload(prepocessing)\n",
    "from typewriter.prepocessing import filter_functions, gen_argument_df_TW, gen_most_frequent_avl_types, \\\n",
    "                                    encode_aval_types_TW\n",
    "from dltpy.input_preparation.generate_df import filter_return_dp, format_df, encode_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters trivial functions such as `__str__` and `__len__` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_functions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts informations for functions' arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = gen_argument_df_TW(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore `self` arguments and args with type of `Any` or `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_count = df_params['arg_name'].count()\n",
    "args_with_annot = df_params[df_params['arg_type'] != ''].shape[0]\n",
    "print(\"Number of arguments: \", args_count)\n",
    "print(\"Args with type annotations: \", args_with_annot)\n",
    "df_params = df_params[(df_params['arg_name'] != 'self') & ((df_params['arg_type'] != 'Any') & \\\n",
    "                                                          (df_params['arg_type'] != 'None'))]\n",
    "print(\"Ignored trivial types: \", (args_count - df_params.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore arguments without a type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = df_params[df_params['arg_type'] != '']\n",
    "print(\"Number of arguments with types: \", df_params.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters out functions:\n",
    "- without a return type\n",
    "- with the return type of `Any` or `None`\n",
    "- without a return expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_return_dp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['arg_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = format_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode types as int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_params, label_encoder, uniq_types = encode_types(df, df_params, TYPES_FILE_TW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add argument names as a string except self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['arg_names_str'] = df['arg_names'].apply(lambda l: \" \".join([v for v in l if v != 'self']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add return expressions as a string, replace self. and self within expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['return_expr_str'] = df['return_expr'].apply(lambda l: \" \".join([re.sub(r\"self\\.?\", '', v) for v in l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all columns useless for the ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['author', 'repo', 'has_type', 'arg_names', 'arg_types', 'arg_descrs', 'return_expr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts top 1000 available types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_TW.CACHE_TW and os.path.exists(os.path.join(AVAILABLE_TYPES_DIR,\n",
    "                                         'top_%d_types.csv' % (config_TW.AVAILABLE_TYPES_NUMBER-1))):\n",
    "    df_types = pd.read_csv(os.path.join(AVAILABLE_TYPES_DIR,\n",
    "                                         'top_%d_types.csv' % (config_TW.AVAILABLE_TYPES_NUMBER-1)))\n",
    "else:\n",
    "    df_types = gen_most_frequent_avl_types(AVAILABLE_TYPES_DIR, config_TW.AVAILABLE_TYPES_NUMBER-1,\n",
    "                                           True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params, df = encode_aval_types_TW(df_params, df, df_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ret_aval_enc'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the datapoints and types coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_enc_types = np.concatenate((df_params['arg_type_enc'].values, df['return_type_enc'].values))\n",
    "other_type_count = np.count_nonzero(all_enc_types == label_encoder.transform(['other'])[0])\n",
    "print(\"Number of datapoints with other types: \", other_type_count)\n",
    "print(\"The percentage of covered unique types: %.2f%%\" % \\\n",
    "      ((config_TW.AVAILABLE_TYPES_NUMBER / len(uniq_types))*100))\n",
    "print(\"The percentage of all datapoints covered by considered types: %.2f%%\" %\\\n",
    "      ((1 - other_type_count / all_enc_types.shape[0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final arguments data before embedding step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final return data before embedding step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits parameters and returns type dataset by file into a train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params_train = df_params[df_params['file'].isin(train_files.to_numpy().flatten())]\n",
    "df_params_test = df_params[df_params['file'].isin(test_files.to_numpy().flatten())]\n",
    "print(df_params_train.shape, df_params_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ret_train = df[df['file'].isin(train_files.to_numpy().flatten())]\n",
    "df_ret_test = df[df['file'].isin(test_files.to_numpy().flatten())]\n",
    "print(df_ret_train.shape, df_ret_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that there is no overlap between the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(df_params_train['file'].tolist()).intersection(set(df_params_test['file'].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(df_ret_train['file'].tolist()).intersection(set(df_ret_test['file'].tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the dataframes and the label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ML_INPUTS_PATH_TW):\n",
    "    os.makedirs(ML_INPUTS_PATH_TW)\n",
    "\n",
    "with open(LABEL_ENCODER_PATH_TW, 'wb') as file:\n",
    "    pickle.dump(label_encoder, file)\n",
    "    \n",
    "#df.to_csv(config.ML_RETURN_DF_PATH_TW, index=False)\n",
    "#df_params.to_csv(config.ML_PARAM_DF_PATH_TW, index=False)\n",
    "df_params_train.to_csv(ML_PARAM_TW_TRAIN, index=False)\n",
    "df_params_test.to_csv(ML_PARAM_TW_TEST, index=False)\n",
    "df_ret_train.to_csv(ML_RET_TW_TRAIN, index=False)\n",
    "df_ret_test.to_csv(ML_RET_TW_TEST, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots 20 most frequent types in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_proc.plot_top_n_types(TYPES_FILE_TW, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typewriter import extraction\n",
    "from typewriter.extraction import EmbeddingTypeWriter\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads dataset for parametes and return types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df = pd.read_csv(ML_PARAM_TW_TRAIN)\n",
    "return_df = pd.read_csv(ML_RET_TW_TRAIN)\n",
    "\n",
    "print(\"Number of parameters types:\", param_df.shape[0])\n",
    "print(\"Number of returns types\", return_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = EmbeddingTypeWriter(param_df, return_df, W2V_MODEL_COMMENTS_DIR,\n",
    "                               W2V_MODEL_TOKEN_DIR)\n",
    "embedder.train_token_model()\n",
    "embedder.train_comment_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads pre-trained W2V models for TypeWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_token_model = Word2Vec.load(W2V_MODEL_TOKEN_DIR)\n",
    "w2v_comments_model = Word2Vec.load(W2V_MODEL_COMMENTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats of the W2V models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"W2V statistics: \")\n",
    "print(\"W2V token model total amount of words : \" + str(w2v_token_model.corpus_total_words))\n",
    "print(\"W2V comments model total amount of words : \" + str(w2v_comments_model.corpus_total_words))\n",
    "print(\"\\n Top 20 words for token model:\")\n",
    "print(w2v_token_model.wv.index2entity[:20])\n",
    "print(\"\\n Top 20 words for comments model:\")\n",
    "print(w2v_comments_model.wv.index2entity[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Vector Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typewriter.extraction import IdentifierSequence, TokenSequence, CommentSequence, \\\n",
    "                                  process_datapoints_TW, gen_aval_types_datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process parameter datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_trans_func_param = lambda row: IdentifierSequence(w2v_token_model, row.arg_name, row.other_args,\n",
    "                                                     row.func_name)\n",
    "token_trans_func_param = lambda row: TokenSequence(w2v_token_model, 7, 3, row.arg_occur, None)\n",
    "\n",
    "cm_trans_func_param = lambda row: CommentSequence(w2v_comments_model, row.func_descr, row.arg_comment, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_ids_param_X_train = process_datapoints_TW(ML_PARAM_TW_TRAIN, VECTOR_OUTPUT_TRAIN,\n",
    "                                             'identifiers_', 'param_train', id_trans_func_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_ids_param_X_test = process_datapoints_TW(ML_PARAM_TW_TEST, VECTOR_OUTPUT_TEST,\n",
    "                                            'identifiers_', 'param_test', id_trans_func_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_tokens_param_X_train = process_datapoints_TW(ML_PARAM_TW_TRAIN, VECTOR_OUTPUT_TRAIN,\n",
    "                                                'tokens_', 'param_train', token_trans_func_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_tokens_param_X_test = process_datapoints_TW(ML_PARAM_TW_TEST, VECTOR_OUTPUT_TEST,\n",
    "                                                'tokens_', 'param_test', token_trans_func_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_cms_param_X_train = process_datapoints_TW(ML_PARAM_TW_TRAIN, VECTOR_OUTPUT_TRAIN,\n",
    "                                            'comments_', 'param_train', cm_trans_func_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_cms_param_X_test = process_datapoints_TW(ML_PARAM_TW_TEST, VECTOR_OUTPUT_TEST,\n",
    "                                            'comments_', 'param_test', cm_trans_func_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Identifiers' train set parameters: \", dp_ids_param_X_train.shape)\n",
    "print(\"Tokens' train set parameters: \", dp_tokens_param_X_train.shape)\n",
    "print(\"Comments' train parameters: \", dp_cms_param_X_train.shape)\n",
    "print(\"Identifiers' test set parameters: \", dp_ids_param_X_test.shape)\n",
    "print(\"Tokens' test set parameters: \", dp_tokens_param_X_test.shape)\n",
    "print(\"Comments' test set parameters: \", dp_cms_param_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process returns datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_trans_func_ret = lambda row: IdentifierSequence(w2v_token_model, None, row.arg_names_str, row.name)\n",
    "\n",
    "token_trans_func_ret = lambda row: TokenSequence(w2v_token_model, 7, 3, None, row.return_expr_str)\n",
    "\n",
    "cm_trans_func_ret = lambda row: CommentSequence(w2v_comments_model, row.func_descr, None, row.return_descr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_ids_ret_X_train = process_datapoints_TW(ML_RET_TW_TRAIN, VECTOR_OUTPUT_TRAIN,\n",
    "                                                     'identifiers_', 'ret_train', id_trans_func_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_ids_ret_X_test = process_datapoints_TW(ML_RET_TW_TEST, VECTOR_OUTPUT_TEST,\n",
    "                                                    'identifiers_', 'ret_test', id_trans_func_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_tokens_ret_X_train = process_datapoints_TW(ML_RET_TW_TRAIN, VECTOR_OUTPUT_TRAIN,\n",
    "                                                        'tokens_', 'ret_train', token_trans_func_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_tokens_ret_X_test = process_datapoints_TW(ML_RET_TW_TEST, VECTOR_OUTPUT_TEST,\n",
    "                                                        'tokens_', 'ret_test', token_trans_func_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_cms_ret_X_train = process_datapoints_TW(ML_RET_TW_TRAIN, VECTOR_OUTPUT_TRAIN, \n",
    "                                                     'comments_', 'ret_train', cm_trans_func_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_cms_ret_X_test = process_datapoints_TW(ML_RET_TW_TEST, VECTOR_OUTPUT_TEST, \n",
    "                                                     'comments_', 'ret_test', cm_trans_func_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Identifiers' train set return: \", dp_ids_ret_X_train.shape)\n",
    "print(\"Identifiers' test set return: \", dp_ids_ret_X_test.shape)\n",
    "print(\"Tokens' train set return: \", dp_tokens_ret_X_train.shape)\n",
    "print(\"Tokens' test set return: \", dp_tokens_ret_X_test.shape)\n",
    "print(\"Comments' train set return:\" , dp_cms_ret_X_train.shape)\n",
    "print(\"Comments' test set return:\" , dp_cms_ret_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates datapoints for available types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_params_train_aval_types, dp_ret_train_aval_types = gen_aval_types_datapoints(ML_PARAM_TW_TRAIN,\n",
    "                                                                                ML_RET_TW_TRAIN,\n",
    "                                                                               'train',\n",
    "                                                                                VECTOR_OUTPUT_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_params_test_aval_types, dp_ret_test_aval_types = gen_aval_types_datapoints(ML_PARAM_TW_TEST,\n",
    "                                                                              ML_RET_TW_TEST,\n",
    "                                                                              'test',\n",
    "                                                                              VECTOR_OUTPUT_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available types-parameters-train:\", dp_params_train_aval_types.shape)\n",
    "print(\"Available types-returns-train:\", dp_ret_train_aval_types.shape)\n",
    "print(\"Available types-parameters-test:\", dp_params_test_aval_types.shape)\n",
    "print(\"Available types-returns-test:\", dp_ret_test_aval_types.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates type vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltpy.input_preparation.df_to_vec import generate_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_y_train, ret_y_train = generate_labels(ML_PARAM_TW_TRAIN, ML_RET_TW_TRAIN,\n",
    "                                              'train', VECTOR_OUTPUT_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_y_test, ret_y_test = generate_labels(ML_PARAM_TW_TEST, ML_RET_TW_TEST,\n",
    "                                            'test', VECTOR_OUTPUT_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Learning the neural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typewriter.model import load_data_tensors_TW, TWModel, train_loop_TW, \\\n",
    "                             evaluate_TW, report_TW, load_label_tensors_TW, \\\n",
    "                             TWModelA, EnhancedTWModel, BaseLineModel\n",
    "from statistics import mean\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"-- Using {device} for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads parameters' data vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_param_train_data():\n",
    "    return load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'identifiers_param_train_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'tokens_param_train_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'comments_param_train_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'params_train_aval_types_dp.npy')), \\\n",
    "           load_label_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'params_train_datapoints_y.npy'))\n",
    "\n",
    "def load_param_test_data():\n",
    "    return load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'identifiers_param_test_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'tokens_param_test_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'comments_param_test_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'params_test_aval_types_dp.npy')), \\\n",
    "           load_label_tensors_TW(join(VECTOR_OUTPUT_TEST, 'params_test_datapoints_y.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads return' data vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ret_train_data():\n",
    "    return load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'identifiers_ret_train_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'tokens_ret_train_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'comments_ret_train_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'ret_train_aval_types_dp.npy')), \\\n",
    "           load_label_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'ret_train_datapoints_y.npy'))\n",
    "\n",
    "def load_ret_test_data():\n",
    "    return load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'identifiers_ret_test_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'tokens_ret_test_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'comments_ret_test_datapoints_x.npy')), \\\n",
    "           load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'ret_test_aval_types_dp.npy')), \\\n",
    "           load_label_tensors_TW(join(VECTOR_OUTPUT_TEST, 'ret_test_datapoints_y.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatanates parameters and return data vectors for combined prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_combined_train_data():\n",
    "    return torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'identifiers_param_train_datapoints_x.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'identifiers_ret_train_datapoints_x.npy')))), \\\n",
    "           torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'tokens_param_train_datapoints_x.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'tokens_ret_train_datapoints_x.npy')))), \\\n",
    "           torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'comments_param_train_datapoints_x.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'comments_ret_train_datapoints_x.npy')))), \\\n",
    "           torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'params_train_aval_types_dp.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'ret_train_aval_types_dp.npy')))), \\\n",
    "           torch.cat((load_label_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'params_train_datapoints_y.npy')),\n",
    "                      load_label_tensors_TW(join(VECTOR_OUTPUT_TRAIN, 'ret_train_datapoints_y.npy'))))\n",
    "\n",
    "def load_combined_test_data():\n",
    "    return torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'identifiers_param_test_datapoints_x.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'identifiers_ret_test_datapoints_x.npy')))), \\\n",
    "           torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'tokens_param_test_datapoints_x.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'tokens_ret_test_datapoints_x.npy')))), \\\n",
    "           torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'comments_param_test_datapoints_x.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'comments_ret_test_datapoints_x.npy')))), \\\n",
    "           torch.cat((load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'params_test_aval_types_dp.npy')),\n",
    "                      load_data_tensors_TW(join(VECTOR_OUTPUT_TEST, 'ret_test_aval_types_dp.npy')))), \\\n",
    "           torch.cat((load_label_tensors_TW(join(VECTOR_OUTPUT_TEST, 'params_test_datapoints_y.npy')),\n",
    "                      load_label_tensors_TW(join(VECTOR_OUTPUT_TEST, 'ret_test_datapoints_y.npy'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_train = {'combined': load_combined_train_data,\n",
    "                  'return': load_ret_train_data,\n",
    "                  'argument': load_param_train_data}\n",
    "datasets_test = {'combined': load_combined_test_data,\n",
    "                 'return': load_ret_test_data,\n",
    "                 'argument': load_param_test_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = config_TW.W2V_VEC_LENGTH\n",
    "hidden_size = 768\n",
    "output_size = 1000\n",
    "num_layers = 1\n",
    "learning_rate = 0.002\n",
    "dropout_rate = 0.25\n",
    "epochs = 25\n",
    "top_n_pred = [1, 3, 5]\n",
    "n_rep = 1\n",
    "batch_size = 2048\n",
    "train_split_size = 0.8\n",
    "data_loader_workers = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete neural model of TypeWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TWModel(input_size, hidden_size, config_TW.AVAILABLE_TYPES_NUMBER, num_layers,\n",
    "                output_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural model of TypeWriter without available types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TWModelA(input_size, hidden_size, num_layers, output_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete neurel model of TypeWriter with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnhancedTWModel(input_size, hidden_size, config_TW.AVAILABLE_TYPES_NUMBER, num_layers,\n",
    "                output_size, dropout_rate).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data parallesim for mutli-GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_other = pickle.load(open(LABEL_ENCODER_PATH_TW, 'rb')).transform(['other'])[0]\n",
    "\n",
    "for d in datasets_train:\n",
    "    print(f\"Loading {d} data for model {model.module.__class__.__name__}\")\n",
    "    #X_id, X_tok, X_cm, X_type, Y = datasets[d]\n",
    "    load_data_t = time.time()\n",
    "    X_id_train, X_tok_train, X_cm_train, X_type_train, Y_train = datasets_train[d]()\n",
    "    X_id_test, X_tok_test, X_cm_test, X_type_test, Y_test = datasets_test[d]()\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_id_train, X_tok_train, X_cm_train, X_type_train,\n",
    "                                            Y_train), batch_size=batch_size, shuffle=True,\n",
    "                                            pin_memory=True, num_workers=data_loader_workers)\n",
    "    \n",
    "    test_loader = DataLoader(TensorDataset(X_id_test, X_tok_test, X_cm_test, X_type_test,\n",
    "                                           Y_test), batch_size=batch_size)\n",
    "    print(\"Loaded train and test sets in %.2f min\" % ((time.time()-load_data_t) / 60))\n",
    "    \n",
    "    for i in range(1, n_rep+1):\n",
    "        \n",
    "        train_t = time.time()\n",
    "        train_loop_TW(model, train_loader, learning_rate, epochs)\n",
    "        print(\"Training finished in %.2f min\" % ((time.time()-train_t) / 60))\n",
    "        eval_t = time.time()\n",
    "        y_true, y_pred = evaluate_TW(model, test_loader, top_n=max(top_n_pred))\n",
    "        print(\"Prediction finished in %.2f min\" % ((time.time()-eval_t) / 60))\n",
    "        \n",
    "        # Ignore other type\n",
    "        idx = (y_true != idx_of_other) & (y_pred[:, 0] != idx_of_other)\n",
    "        f1_score_top_n = []\n",
    "        for top_n in top_n_pred:\n",
    "            filename = f\"{model.module.__class__.__name__}_{d}_{i}_{top_n}\"\n",
    "            report_TW(y_true, y_pred, top_n, f\"{filename}_unfiltered\", RESULTS_DIR)\n",
    "            report = report_TW(y_true[idx], y_pred[idx], top_n, f\"{filename}_filtered\", RESULTS_DIR)\n",
    "            f1_score_top_n.append(report['macro avg']['f1-score'])\n",
    "        print(\"Mean f1_score:\", mean(f1_score_top_n))\n",
    "        \n",
    "        model.module.reset_model_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_other = pickle.load(open(LABEL_ENCODER_PATH_TW, 'rb')).transform(['other'])[0]\n",
    "\n",
    "baseline_model = BaseLineModel(TYPES_FILE_TW)\n",
    "\n",
    "for d in datasets_train:\n",
    "    print(f\"Loading {d} data for model {baseline_model.__class__.__name__}\")\n",
    "   \n",
    "    X_id_test, X_tok_test, X_cm_test, X_type_test, y_test = datasets_test[d]()\n",
    "    y_test = y_test.numpy()\n",
    "    \n",
    "    y_pred = baseline_model.predict(X_id_test)\n",
    "    \n",
    "    # Ignore other type\n",
    "    idx = (y_test != idx_of_other) & (y_pred[:, 0] != idx_of_other)\n",
    "    f1_score_top_n = []\n",
    "    for top_n in top_n_pred:\n",
    "        filename = f\"{baseline_model.__class__.__name__}_{d}_{1}_{top_n}\"\n",
    "        report_TW(y_test, y_pred, top_n, f\"{filename}_unfiltered\", RESULTS_DIR)\n",
    "        report = report_TW(y_test[idx], y_pred[idx], top_n, f\"{filename}_filtered\", RESULTS_DIR)\n",
    "        print(report['weighted avg'])\n",
    "        f1_score_top_n.append(report['weighted avg']['f1-score'])\n",
    "    print(\"Mean f1_score:\", mean(f1_score_top_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import result_proc\n",
    "reload(result_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = result_proc.eval_result(RESULTS_DIR, 'EnhancedTWModel', 'return', 'filtered', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_proc.plot_result(res, \"NaiveBaseline-Return-MacroAvg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_proc.copy_results(RESULTS_DIR, './results/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RayTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "import ray\n",
    "ray.init(memory=16 * 1024 * 1024 * 1024,\n",
    "         object_store_memory=8 * 1024 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = tune.utils.pin_in_object_store(train_loader), tune.utils.pin_in_object_store(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_other = tune.utils.pin_in_object_store(idx_of_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ray.remote\n",
    "def train_TW(config):\n",
    "    top_n_pred = [1, 3, 5]\n",
    "    model = learn.TWModel(input_size, config['hidden_size'], X_types_param.shape[1],\n",
    "                          config['num_layers'], output_size, True).to(device)\n",
    "\n",
    "    #for i in range(1, n_rep+1):\n",
    "    i=1\n",
    "\n",
    "    learn.train_loop_TW(model, config['train_loader'], config['learning_rate'], config['epochs'])\n",
    "    y_true, y_pred = learn.evaluate_TW(model, config['test_loader'], top_n=max(top_n_pred))\n",
    "#     learn.train_loop_TW(model, train_loader, config['learning_rate'], config['epochs'])\n",
    "#     y_true, y_pred = learn.evaluate_TW(model, test_loader, top_n=max(top_n_pred))\n",
    "\n",
    "    # Ignore other type\n",
    "    #idx_of_other = pickle.load(open(f'./output/ml_inputs/label_encoder.pkl', 'rb')).transform(['other'])[0]\n",
    "    idx = (y_true != tune.utils.get_pinned_object(idx_of_other)) & (y_pred[:, 0] != tune.utils.get_pinned_object(idx_of_other))\n",
    "    f1_score_top_n = []\n",
    "    for top_n in top_n_pred:\n",
    "        filename = f\"{learn.TWModel.__name__}_complete_{i}_{top_n}\"\n",
    "        #learn.report_TW(y_true, y_pred, top_n, f\"{filename}_unfiltered\")\n",
    "        report = learn.report_TW(y_true[idx], y_pred[idx], top_n, y_true.shape[0], f\"{filename}_filtered\")\n",
    "        f1_score_top_n.append(report['weighted avg']['f1-score'])\n",
    "    print(\"Mean f1_score:\", mean(f1_score_top_n))\n",
    "    ray.tune.track.log(mean_f1_score=mean(f1_score_top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(train_TW,\n",
    "           config={'hidden_size': tune.grid_search([32, 64, 128]),\n",
    "                   'num_layers': tune.grid_search([1]),\n",
    "                   'learning_rate': tune.grid_search([0.002]),\n",
    "                   'epochs': tune.grid_search([5]),\n",
    "                   'train_loader': train_loader,\n",
    "                   'test_loader': test_loader},\n",
    "                   name=\"TypeWriter_model\",\n",
    "                   resources_per_trial={\"cpu\": 2,\n",
    "                                        \"gpu\": 2},\n",
    "                   verbose=1)\n",
    "print(\"Best config: \", analysis.get_best_config(metric=\"mean_f1_score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
